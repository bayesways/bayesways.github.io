<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>How to try different llm models | Konstantinos Vamvourellis</title> <meta name="author" content="Konstantinos Vamvourellis"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://bayesways.github.io/blog/2025/How-I-experiment-with-different-LLM-models/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Konstantinos </span>Vamvourellis</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">How to try different llm models</h1> <p class="post-meta">May 16, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llm"> <i class="fas fa-hashtag fa-sm"></i> llm</a>   </p> </header> <article class="post-content"> <h3 id="why-llm-is-a-great-package-for-automation-and-productivity">Why LLM is a Great Package for Automation and Productivity</h3> <p>The <a href="https://llm.datasette.io/en/stable/index.html" rel="external nofollow noopener" target="_blank">LLM package by Simon Willison</a> offers a unique combination of features that make it an ideal tool for research, automation and productivity. Kudos to Simon for making such a powerful piece of software. In this article, I’ll explore three reasons to use it.</p> <h3 id="reason-1-easy-experimentation-with-different-llms">Reason 1: Easy Experimentation with Different LLMs</h3> <p>One of the most significant benefits of the LLM package (LLM) is its ability to make it easy to experiment with different LLM models. With LLM, you can store all your results and revisit them later to examine differences or find answers to questions you’ve previously asked. This feature is particularly useful for exploring the capabilities of different LLMs and identifying the best one for your specific use case.</p> <p>There are other good ways to run different llm models online, such as <a href="https://app.hyperbolic.xyz" rel="external nofollow noopener" target="_blank">hyperbolic</a> or hugging face <a href="https://huggingface.co/playground" rel="external nofollow noopener" target="_blank">inference playground</a>, and some to run them locally, such as <a href="https://lmstudio.ai/" rel="external nofollow noopener" target="_blank">LM Studio</a>. And of course there are all the chatbot interfaces from OpenAI, Anthropic, Google AI Studio etc. which are truly fantastic products. But if you are interested in trying them all out, there are actually too many places which makes tracking your usage too hard. The LLM package solves this problem for me to a good degree. I use it to interact with all the API services and all open source models including all local models. This way I am capturing all the interactions automatically. All prompts and outputs are <a href="https://llm.datasette.io/en/stable/logging.html" rel="external nofollow noopener" target="_blank">stored automatically</a> in a local database together with all the metadata. LLM has a variety of plugins for many API services and local inference frameworks. For the rare occasion that there is an API that is not covered, you can write your own plugin (Simon has kindly provided a detailed guide on how to do exactly that - and if you do, others will benefit too!). What I cannot capture are my interactions with ChatGPT or other chatbot interfaces, but at least for those there is a history saved on the interface.</p> <h3 id="reason-2-easy-local-model-running">Reason 2: Easy Local Model Running</h3> <p>LLM also makes it very easy to run models locally on your computer. With LLM, you can access a wide range of local inference frameworks, all within a single package. Each one is made available via an <code class="language-plaintext highlighter-rouge">llm</code>-plugin that can be easily installed via <code class="language-plaintext highlighter-rouge">pip</code>. You can find the full list <a href="https://llm.datasette.io/en/stable/plugins/directory.html" rel="external nofollow noopener" target="_blank">here</a>. This feature is particularly useful for those who want to run models on their own computer without incurring costs or relying on external cloud services. No API keys or wifi needed. What is more, all data stays with the user, which is something I appreciate a lot. Below I am showing how to run <a href="https://huggingface.co/mlx-community/Llama-3.2-3B-Instruct-4bit" rel="external nofollow noopener" target="_blank">Llama3-3B-4bit</a> locally. This model is powerful enough for common tasks, yet small and very performant. Combined with the mlx inference framework it is possible to run this model easily on the background without having your computer slow down if you are running your regular apps in parallel.</p> <p><strong>Running Lama3 Locally on Your Mac</strong> <strong>System Requirements</strong></p> <ul> <li>Mac Pro (or a MacBook Pro machine)</li> <li>Local folder with virtual environment and local python version 3.12. We specifically need 3.12 and not higher. See my notes on <a href="2025-05-14-How%20I%20set%20up%20my%20project%20repositories.md">“How I set up my project repositories”</a>.</li> <li>Enter your project repo and activate the virtual environment</li> </ul> <p><strong>Step 1: Install llm</strong> Follow the <a href="https://llm.datasette.io/en/stable/setup.html" rel="external nofollow noopener" target="_blank">instructions</a> for setting up llm. Inside the virtual environment we can use pip.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> <td class="rouge-code"><pre>pip <span class="nb">install </span>llm
</pre></td> </tr></tbody></table></code></pre></div></div> <p><strong>Step 2: Install llm-mlx</strong> Follow the <a href="https://github.com/simonw/llm-mlx?tab=readme-ov-file" rel="external nofollow noopener" target="_blank">instructions</a> of the llm-mlx plugin. Since we have setup our virtual environment with python 3.12 already we don’t need to follow the special instructions listed. The following one line will do.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> <td class="rouge-code"><pre>llm <span class="nb">install </span>llm-mlx
</pre></td> </tr></tbody></table></code></pre></div></div> <p><strong>Step 3: Install the Llama model</strong> This will take a minute or two to download.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> <td class="rouge-code"><pre>llm mlx download-model mlx-community/Llama-3.2-3B-Instruct-4bit
</pre></td> </tr></tbody></table></code></pre></div></div> <p><strong>Step 4: Test it out</strong> Try prompting the model with a simple query</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> <td class="rouge-code"><pre>llm <span class="nt">-m</span> mlx-community/Llama-3.2-3B-Instruct-4bit <span class="s1">'Capital of France?'</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <ul> <li>You can run other local models as listed on the <a href="llm%20-m%20mlx-community/Llama-3.2-3B-Instruct-4bit" title="Capital of France?' -s 'you are a pelican">repo</a> of the plugin on github</li> <li>To delete a downloaded model amd free space on your computer follow the instructions <a href="https://github.com/simonw/llm-mlx/issues/14" rel="external nofollow noopener" target="_blank">here</a> </li> </ul> <h3 id="reason-3-customizable-workflows-in-the-terminal">Reason 3: Customizable Workflows in the Terminal</h3> <p>Finally, the fact that LLM is integrated into the terminal, combined with a variety of helpful built in features, allow us to create custom little workflows to automate various everyday tasks. For example I used LLM and Llama3 to write this blog post. Here is how.</p> <h3 id="example">Example</h3> <p><strong>Using llama3B to Automate Writing Tasks</strong></p> <p>First I ran the following command to create a chat template (a great LLM feature). This stores a preference for a model and a system prompt I want to reuse in the future.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> <td class="rouge-code"><pre>llm llm <span class="nt">-m</span> mlx-community/Llama-3.2-3B-Instruct-4bit <span class="nt">-s</span> <span class="s2">"You are my helpful copyright editor that writes for my AI blog. You can take in some quick notes that I give you and you turn it into clear text. If you don't have all the details to complete the article you put placeholders for me to fill in later. You use simple language in a neutral and professional tone. You do not hype the topics or get too excited. You output only markdown files."</span> <span class="nt">--save</span> copywriter
</pre></td> </tr></tbody></table></code></pre></div></div> <p>Then I started a chat with this template</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> <td class="rouge-code"><pre>llm chat <span class="nt">-t</span> copywriter
</pre></td> </tr></tbody></table></code></pre></div></div> <p>I went back a forth a few times with the model to write different pieces of the post, one prompt at a time. I didn’t get a perfect response each time, but within 5 iterations I got what I needed to put the post together. I had to edit it manually for 10 mins before completing it.</p> <p><strong>Tips and Modifications</strong> By combining LLM features with a dictation app, you can replace typing by speaking when prompting which makes the process a lot faster. This can be particularly useful for tasks that require a lot of text input, such as writing emails, creating reports, or drafting documents. I use <a href="https://goodsnooze.gumroad.com/l/macwhisper" rel="external nofollow noopener" target="_blank">Macwhisperer</a> which runs locally. This way I have a pipeline that runs on my laptop, for free, requires no internet access and no data leaves my computer.</p> <p>Simon has added a ton of useful features to the LLM package which make it easy to automate different workflows. You can take some ideas from his <a href="https://www.youtube.com/watch?v=QUXQNi6jQ30" rel="external nofollow noopener" target="_blank">video demo</a>, or check out the <a href="https://llm.datasette.io/en/stable/plugins/index.html" rel="external nofollow noopener" target="_blank">list</a> of tools and plugins available on the documentation.</p> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"bayesways/bayesways.github.io","data-repo-id":"R_kgDOJEHZog","data-category":"Announcements","data-category-id":"DIC_kwDOJEHZos4CUlC1","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Konstantinos Vamvourellis. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>