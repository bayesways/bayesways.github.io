<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Local LLama2+Langchain on a macbook pro | Konstantinos Vamvourellis</title> <meta name="author" content="Konstantinos Vamvourellis"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://bayesways.github.io/blog/2023/Local-LLama2-Langchain-on-a-macbook-pro/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Konstantinos </span>Vamvourellis</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">talks</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Local LLama2+Langchain on a macbook pro</h1> <p class="post-meta">July 24, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/llm"> <i class="fas fa-hashtag fa-sm"></i> llm</a>   </p> </header> <article class="post-content"> <p>In this post I will show how to build a simple LLM chain that runs completely locally on your macbook pro. Will use the latest Llama2 models with Langchain.</p> <h1 id="run-locally-on-your-macbook-pro">Run locally on your Macbook Pro</h1> <p>Create a directory to put all the models and code notebooks in</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td> <td class="rouge-code"><pre><span class="nb">mkdir </span>llama2
<span class="nb">cd </span>llama2
</pre></td> </tr></tbody></table></code></pre></div></div> <p>then follow the instructions by <a href="https://blog.lastmileai.dev/run-llama-2-locally-in-7-lines-apple-silicon-mac-c3f46143f327" rel="external nofollow noopener" target="_blank">Suyog Sonwalkar</a> copied here for convenience:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="rouge-code"><pre>git clone https://github.com/ggerganov/llama.cpp.git  
<span class="nb">cd </span>llama.cpp  
curl <span class="nt">-L</span> https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_K_M.bin <span class="nt">--output</span> ./models/llama-2-7b-chat.ggmlv3.q4_K_M.bin   
<span class="nv">LLAMA_METAL</span><span class="o">=</span>1 make
</pre></td> </tr></tbody></table></code></pre></div></div> <p>This will save the local model at the following path (we will need this path later)</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> <td class="rouge-code"><pre>"llama2/llama.cpp/models/llama-2-7b-chat.ggmlv3.q4_K_M.bin",
</pre></td> </tr></tbody></table></code></pre></div></div> <p>You can test the model works by telling it “recipe for avocado toast”. To do that go to the <code class="language-plaintext highlighter-rouge">llama.cpp</code> directory (if you are not already) and do</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> <td class="rouge-code"><pre>./main <span class="nt">-m</span> ./models/llama-2-7b-chat.ggmlv3.q4_K_M.bin <span class="nt">-n</span> 1024 <span class="nt">-ngl</span> 1 <span class="nt">-p</span> <span class="s2">"recipe for avocado toast"</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <p>You should get something like this:</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
</pre></td> <td class="rouge-code"><pre>main: build <span class="o">=</span> 893 <span class="o">(</span>4f06592<span class="o">)</span>
main: seed  <span class="o">=</span> 1690215975
llama.cpp: loading model from ./models/llama-2-7b-chat.ggmlv3.q4_K_M.bin
llama_model_load_internal: format     <span class="o">=</span> ggjt v3 <span class="o">(</span>latest<span class="o">)</span>
llama_model_load_internal: n_vocab    <span class="o">=</span> 32000
llama_model_load_internal: n_ctx      <span class="o">=</span> 512
llama_model_load_internal: n_embd     <span class="o">=</span> 4096
llama_model_load_internal: n_mult     <span class="o">=</span> 256
llama_model_load_internal: n_head     <span class="o">=</span> 32
llama_model_load_internal: n_head_kv  <span class="o">=</span> 32
llama_model_load_internal: n_layer    <span class="o">=</span> 32
llama_model_load_internal: n_rot      <span class="o">=</span> 128
llama_model_load_internal: n_gqa      <span class="o">=</span> 1
llama_model_load_internal: n_ff       <span class="o">=</span> 11008
llama_model_load_internal: freq_base  <span class="o">=</span> 10000.0
llama_model_load_internal: freq_scale <span class="o">=</span> 1
llama_model_load_internal: ftype      <span class="o">=</span> 15 <span class="o">(</span>mostly Q4_K - Medium<span class="o">)</span>
llama_model_load_internal: model size <span class="o">=</span> 7B
llama_model_load_internal: ggml ctx size <span class="o">=</span>    0.08 MB
llama_model_load_internal: mem required  <span class="o">=</span> 4193.33 MB <span class="o">(</span>+  256.00 MB per state<span class="o">)</span>
llama_new_context_with_model: kv self size  <span class="o">=</span>  256.00 MB
ggml_metal_init: allocating
ggml_metal_init: using MPS
ggml_metal_init: loading <span class="s1">'/Users/Konstantinosvamvourellis/llama2/llama.cpp/ggml-metal.metal'</span>
ggml_metal_init: loaded kernel_add                            0x107007530
ggml_metal_init: loaded kernel_add_row                        0x107007d60
ggml_metal_init: loaded kernel_mul                            0x107008280
ggml_metal_init: loaded kernel_mul_row                        0x1070088b0
ggml_metal_init: loaded kernel_scale                          0x107008dd0
ggml_metal_init: loaded kernel_silu                           0x1070092f0
ggml_metal_init: loaded kernel_relu                           0x107009810
ggml_metal_init: loaded kernel_gelu                           0x107009d30
ggml_metal_init: loaded kernel_soft_max                       0x10700a3e0
ggml_metal_init: loaded kernel_diag_mask_inf                  0x10700aa40
ggml_metal_init: loaded kernel_get_rows_f16                   0x10700b0c0
ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10700b8b0
ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10700bf30
ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10700c5b0
ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10700cc30
ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10700d2b0
ggml_metal_init: loaded kernel_get_rows_q5_K                  0x10700d930
ggml_metal_init: loaded kernel_get_rows_q6_K                  0x10700dfb0
ggml_metal_init: loaded kernel_rms_norm                       0x10700e670
ggml_metal_init: loaded kernel_norm                           0x10700ee90
ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x10700f6f0
ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10700fdb0
ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x107010470
ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x107010cb0
ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x107011370
ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x107011a30
ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1070120d0
ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x107012bd0
ggml_metal_init: loaded kernel_rope                           0x1070130f0
ggml_metal_init: loaded kernel_alibi_f32                      0x1070139b0
ggml_metal_init: loaded kernel_cpy_f32_f16                    0x107014240
ggml_metal_init: loaded kernel_cpy_f32_f32                    0x107014ad0
ggml_metal_init: loaded kernel_cpy_f16_f16                    0x107015240
ggml_metal_init: recommendedMaxWorkingSetSize <span class="o">=</span> 10922.67 MB
ggml_metal_init: hasUnifiedMemory             <span class="o">=</span> <span class="nb">true
</span>ggml_metal_init: maxTransferRate              <span class="o">=</span> built-in GPU
llama_new_context_with_model: max tensor size <span class="o">=</span>   102.54 MB
ggml_metal_add_buffer: allocated <span class="s1">'data            '</span> buffer, size <span class="o">=</span>  3891.69 MB, <span class="o">(</span> 3892.14 / 10922.67<span class="o">)</span>
ggml_metal_add_buffer: allocated <span class="s1">'eval            '</span> buffer, size <span class="o">=</span>    10.00 MB, <span class="o">(</span> 3902.14 / 10922.67<span class="o">)</span>
ggml_metal_add_buffer: allocated <span class="s1">'kv              '</span> buffer, size <span class="o">=</span>   258.00 MB, <span class="o">(</span> 4160.14 / 10922.67<span class="o">)</span>
ggml_metal_add_buffer: allocated <span class="s1">'scr0            '</span> buffer, size <span class="o">=</span>   132.00 MB, <span class="o">(</span> 4292.14 / 10922.67<span class="o">)</span>
ggml_metal_add_buffer: allocated <span class="s1">'scr1            '</span> buffer, size <span class="o">=</span>   160.00 MB, <span class="o">(</span> 4452.14 / 10922.67<span class="o">)</span>

system_info: n_threads <span class="o">=</span> 6 / 10 | AVX <span class="o">=</span> 0 | AVX2 <span class="o">=</span> 0 | AVX512 <span class="o">=</span> 0 | AVX512_VBMI <span class="o">=</span> 0 | AVX512_VNNI <span class="o">=</span> 0 | FMA <span class="o">=</span> 0 | NEON <span class="o">=</span> 1 | ARM_FMA <span class="o">=</span> 1 | F16C <span class="o">=</span> 0 | FP16_VA <span class="o">=</span> 1 | WASM_SIMD <span class="o">=</span> 0 | BLAS <span class="o">=</span> 1 | SSE3 <span class="o">=</span> 0 | VSX <span class="o">=</span> 0 |
sampling: repeat_last_n <span class="o">=</span> 64, repeat_penalty <span class="o">=</span> 1.100000, presence_penalty <span class="o">=</span> 0.000000, frequency_penalty <span class="o">=</span> 0.000000, top_k <span class="o">=</span> 40, tfs_z <span class="o">=</span> 1.000000, top_p <span class="o">=</span> 0.950000, typical_p <span class="o">=</span> 1.000000, temp <span class="o">=</span> 0.800000, mirostat <span class="o">=</span> 0, mirostat_lr <span class="o">=</span> 0.100000, mirostat_ent <span class="o">=</span> 5.000000
generate: n_ctx <span class="o">=</span> 512, n_batch <span class="o">=</span> 512, n_predict <span class="o">=</span> 1024, n_keep <span class="o">=</span> 0


 recipe <span class="k">for </span>avocado toast with fresh herbs and lemon

This is a simple and delicious recipe <span class="k">for </span>avocado toast that incorporates fresh herbs and a squeeze of lemon. The addition of the herbs gives the dish a bright, refreshing flavor that complements the richness of the avocado.
Ingredients:

<span class="k">*</span> 2 slices of bread <span class="o">(</span>preferably whole wheat or whole grain<span class="o">)</span>
<span class="k">*</span> 1 ripe avocado, mashed
<span class="k">*</span> Fresh herbs <span class="o">(</span>such as parsley, basil, or cilantro<span class="o">)</span> chopped
<span class="k">*</span> Salt and pepper to taste
<span class="k">*</span> Lemon wedges <span class="o">(</span>optional<span class="o">)</span>
Instructions:

1. Toast the bread <span class="k">until </span>it is lightly browned.
2. Spread the mashed avocado on top of the toasted bread.
3. Sprinkle the chopped fresh herbs over the avocado.
4. Season with salt and pepper to taste.
5. Squeeze a slice of lemon over the avocado toast, <span class="k">if </span>desired.
6. Serve immediately and enjoy!

This recipe is easy to make and can be customized to your liking by using different types of bread or herbs. The lemon adds a <span class="nb">nice </span>tanginess to the dish, but feel free to omit it <span class="k">if </span>you prefer. Enjoy! <span class="o">[</span>end of text]

llama_print_timings:        load <span class="nb">time</span> <span class="o">=</span>  5282.86 ms
llama_print_timings:      sample <span class="nb">time</span> <span class="o">=</span>   200.57 ms /   308 runs   <span class="o">(</span>    0.65 ms per token,  1535.59 tokens per second<span class="o">)</span>
llama_print_timings: prompt <span class="nb">eval time</span> <span class="o">=</span>   637.46 ms /     9 tokens <span class="o">(</span>   70.83 ms per token,    14.12 tokens per second<span class="o">)</span>
llama_print_timings:        <span class="nb">eval time</span> <span class="o">=</span>  9804.24 ms /   307 runs   <span class="o">(</span>   31.94 ms per token,    31.31 tokens per second<span class="o">)</span>
llama_print_timings:       total <span class="nb">time</span> <span class="o">=</span> 10668.64 ms
ggml_metal_free: deallocating
</pre></td> </tr></tbody></table></code></pre></div></div> <p>The model output is this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre></td> <td class="rouge-code"><pre> recipe for avocado toast with fresh herbs and lemon

This is a simple and delicious recipe for avocado toast that incorporates fresh herbs and a squeeze of lemon. The addition of the herbs gives the dish a bright, refreshing flavor that complements the richness of the avocado.
Ingredients:

* 2 slices of bread (preferably whole wheat or whole grain)
* 1 ripe avocado, mashed
* Fresh herbs (such as parsley, basil, or cilantro) chopped
* Salt and pepper to taste
* Lemon wedges (optional)
Instructions:

1. Toast the bread until it is lightly browned.
2. Spread the mashed avocado on top of the toasted bread.
3. Sprinkle the chopped fresh herbs over the avocado.
4. Season with salt and pepper to taste.
5. Squeeze a slice of lemon over the avocado toast, if desired.
6. Serve immediately and enjoy!

This recipe is easy to make and can be customized to your liking by using different types of bread or herbs. The lemon adds a nice tanginess to the dish, but feel free to omit it if you prefer. Enjoy! [end of text]
</pre></td> </tr></tbody></table></code></pre></div></div> <p>In this example we installed the LLama2-7B param model for chat. Later I will show how to do the same for the bigger Llama2 models. To see all the LLM model versions that Meta has released on hugging face go <a href="https://huggingface.co/meta-llama" rel="external nofollow noopener" target="_blank">here</a>. Note that to use any of these models from hugging face you’ll need to request approval using this <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" rel="external nofollow noopener" target="_blank">form</a>. You can do that following this <a href="https://twitter.com/jamescalam/status/1682766618831777794?s=61&amp;t=Pw-aY--IwGlNpRBvUW-P9g" rel="external nofollow noopener" target="_blank">demo</a> by James Briggs.</p> <p>To run the model locally though you’ll need the quantized versions of the model (in order to fit within the limitations of your macbook pro). These versions have been made available by <a href="https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main" rel="external nofollow noopener" target="_blank">TheBloke</a> (based on the work by <a href="https://github.com/ggerganov/llama.cpp" rel="external nofollow noopener" target="_blank">ggerganov</a>), and that’s what we are using here.</p> <h1 id="run-with-langchain">Run with Langchain</h1> <p>To use your local model with Langchain follow the langchain documentation <a href="https://python.langchain.com/docs/use_cases/question_answering/how_to/local_retrieval_qa#llama-v2" rel="external nofollow noopener" target="_blank">here</a>.</p> <p>Note that you’ll need to pass the correct path to your model bin file in the LLamaCpp <code class="language-plaintext highlighter-rouge">model_path</code> param . For me this path was <code class="language-plaintext highlighter-rouge">"../llama.cpp/models/llama-2-7b-chat.ggmlv3.q4_K_M.bin"</code> as shown below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td> <td class="rouge-code"><pre><span class="c1"># Make sure the model path is correct for your system!
</span><span class="n">llm</span> <span class="o">=</span> <span class="nc">LlamaCpp</span><span class="p">(</span>
    <span class="n">model_path</span><span class="o">=</span><span class="sh">"</span><span class="s">../llama.cpp/models/llama-2-7b-chat.ggmlv3.q4_K_M.bin</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">n_gpu_layers</span><span class="o">=</span><span class="n">n_gpu_layers</span><span class="p">,</span>
    <span class="n">n_batch</span><span class="o">=</span><span class="n">n_batch</span><span class="p">,</span>
    <span class="n">n_ctx</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">f16_kv</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># MUST set to True, otherwise you will run into problem after a couple of calls
</span>    <span class="n">callback_manager</span><span class="o">=</span><span class="n">callback_manager</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <h1 id="running-other-llama-model-versions">Running other LLama model versions</h1> <p>LLama has released different versions of the models, as discussed. However to run them locally you’ll need the quantized versions provided by independent developers such as <a href="https://huggingface.co/TheBloke" rel="external nofollow noopener" target="_blank">TheBloke</a>.</p> <p>To download other versions you’ll need to</p> <ul> <li>find the appropriate quantized version you’ll need, for example go <a href="https://huggingface.co/TheBloke/Llama-2-70B-GGML" rel="external nofollow noopener" target="_blank">here</a> for the 70B param GGML model version</li> <li>adjust the names of the models in the commands used, for example replace <code class="language-plaintext highlighter-rouge">llama-2-7b-chat.ggmlv3.q4_K_M.bin</code> with <code class="language-plaintext highlighter-rouge">llama-2-70b.ggmlv3.q4_0.bin</code> </li> </ul> <p>For example, I was able to install the 13B chat version and ran it successfully. To do that and test that it works go to the <code class="language-plaintext highlighter-rouge">llama.cpp</code> directory (if you are not already) and do</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre></td> <td class="rouge-code"><pre>
curl <span class="nt">-L</span> https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin <span class="nt">--output</span> ./models/llama-2-13b-chat.ggmlv3.q4_0.bin   
<span class="nv">LLAMA_METAL</span><span class="o">=</span>1 make
./main <span class="nt">-m</span> ./models/llama-2-13b-chat.ggmlv3.q4_0.bin <span class="nt">-n</span> 1024 <span class="nt">-ngl</span> 1 <span class="nt">-p</span> <span class="s2">"recipe for avocado toast"</span>
</pre></td> </tr></tbody></table></code></pre></div></div> <h1 id="performance">Performance</h1> <p>I was able to run the 7B chat version model on my machine - see this <a href="https://github.com/bayesways/local_llama2_demo" rel="external nofollow noopener" target="_blank">repo</a> for my setup. I ran it on a Macbook pro with an M2 Pro chip 16G RAM and it took ~1-2 seconds to generate the response. The performance you’ll experience will be a function of the parameters used. To see all the parameters go to <code class="language-plaintext highlighter-rouge">llama.cpp</code> directory and do</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
</pre></td> <td class="rouge-code"><pre>./main --help
</pre></td> </tr></tbody></table></code></pre></div></div> <p>I found performance to be sensitive to the context size (<code class="language-plaintext highlighter-rouge">--ctx-size</code> in terminal, <code class="language-plaintext highlighter-rouge">n_ctx</code> in langchain) in Langchain but less so in the terminal. If you are getting a slow response try lowering the context size <code class="language-plaintext highlighter-rouge">n_ctx</code>.</p> <p>Here are the performance metadata from the terminal calls for the two models:</p> <p>Performance of the 7B model:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td> <td class="rouge-code"><pre>llama_print_timings:        load time =  4910.88 ms
llama_print_timings:      sample time =   154.19 ms /   236 runs   (    0.65 ms per token,  1530.57 tokens per second)
llama_print_timings: prompt eval time =   491.16 ms /     9 tokens (   54.57 ms per token,    18.32 tokens per second)
llama_print_timings:        eval time =  7490.65 ms /   235 runs   (   31.88 ms per token,    31.37 tokens per second)
llama_print_timings:       total time =  8155.16 ms
</pre></td> </tr></tbody></table></code></pre></div></div> <p>Performance of the 13B model:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr>
<td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td> <td class="rouge-code"><pre>llama_print_timings:        load time =  8227.97 ms
llama_print_timings:      sample time =   331.46 ms /   506 runs   (    0.66 ms per token,  1526.60 tokens per second)
llama_print_timings: prompt eval time =  8755.93 ms /   266 tokens (   32.92 ms per token,    30.38 tokens per second)
llama_print_timings:        eval time = 28269.76 ms /   504 runs   (   56.09 ms per token,    17.83 tokens per second)
llama_print_timings:       total time = 37405.66 ms
</pre></td> </tr></tbody></table></code></pre></div></div> <h1 id="run-online-in-google-colabhugging-face">Run Online in Google Colab+Hugging Face</h1> <p>If you don’t have a macbook with a M2 chip or want faster performance you can run llama2 with langchain in a google colab <a href="https://github.com/bayesways/local_llama2_demo/blob/main/llama_2_13b_chat_agent.ipynb" rel="external nofollow noopener" target="_blank">notebook</a>. I’ve taken the instructions from this <a href="https://twitter.com/jamescalam/status/1682766618831777794?s=61&amp;t=Pw-aY--IwGlNpRBvUW-P9g" rel="external nofollow noopener" target="_blank">demo</a> by James Briggs and simplified. The notebook runs from start to finish in ~ 10 mins.</p> <p>Since we are using Google Colab the model is running on the GPUs that Google makes available as part of the Colab service. I found that 13b parameter Llama2 model runs fast and stays within the GPU memory limits of the free Colab version. The 70B parameter is worth trying but you might need to upgrade the resources and with a paid subscription for that. We also make use of the the Hugging face pipeline from Langchain which allows to run any model published in Hugging Face, which makes for more general code, but for that you will need any API key.</p> <p>The setup you need is as follows:</p> <ul> <li>Hugging face api key</li> <li>Save the Hugging face api key in a file <code class="language-plaintext highlighter-rouge">secret_api_keys.env</code> which you’ll save in your top directory of the google drive.</li> <li>To apply to use LLama2 and get approved (go <a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/" rel="external nofollow noopener" target="_blank">here</a> and register with an email same as the one in your hugging face account)</li> <li>Hugging face to match your Meta approval with your Hugging face email</li> </ul> <h2 id="references">References</h2> <ul> <li>Llama2+Langchain <a href="https://twitter.com/jamescalam/status/1682766618831777794?s=61&amp;t=Pw-aY--IwGlNpRBvUW-P9g" rel="external nofollow noopener" target="_blank">demo</a> by James Briggs</li> <li>Run LLama2 on macbook pro based on three similar instructions by <a href="https://twitter.com/AdrienBrault/status/1681503574814081025" rel="external nofollow noopener" target="_blank">Adrien Brault</a>, <a href="https://blog.lastmileai.dev/run-llama-2-locally-in-7-lines-apple-silicon-mac-c3f46143f327" rel="external nofollow noopener" target="_blank">Suyog Sonwalkar</a> or <a href="https://www.youtube.com/watch?v=Kg588OVYTiw" rel="external nofollow noopener" target="_blank">Abhishek Thakur</a> </li> <li>Langchain <a href="https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa" rel="external nofollow noopener" target="_blank">documentation</a> for locally run models</li> </ul> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"bayesways/bayesways.github.io","data-repo-id":"R_kgDOJEHZog","data-category":"Announcements","data-category-id":"DIC_kwDOJEHZos4CUlC1","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Konstantinos Vamvourellis. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>